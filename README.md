<div align="center">
    <h1>Awesome Spatial Intelligence in MLLM</h1>
    <img src=https://img.shields.io/github/stars/ZYCheng1002/Awesome-Spatial-Intelligence-MLLM.svg?style=social >
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg"/></a>
</div>

This is a collection of research papers about Spatial Intelligence in MLLM. 

If you wish to include your paper, update any details (e.g., code URLs, conference information), or share suggestions, please feel free to submit a pull request, send me an email, or leave a comment in the Issues section. Your input is greatly appreciated!


* Methods  
Focus on collecting relevant work centered around model training. Some may include datasets and benchmarks, which will be annotated as much as possible.
* Dataset  
Focus on collecting datasets related to spatial intelligence. Some may involve SFT (Supervised Fine-Tuning) processes.
* Benchmark  
Focus on collecting relevant evaluation benchmarks.


## Methods
|              Title              |                    Introduction                    |     Date     |      Code      |    Dataset Open    |  Benchmark  |
| :-----------------------------: | :------------------------------------------------: | :----------: | :------------: | :----------------: | :---------: |
| <br/>[RoboBrain 2.0: See Better. Think Harder. Do Smarter.](https://arxiv.org/abs/2507.02029) | <img width="300" alt="image" src="images/RoboBrain2-0.png"> | 2025-8 | [Github](https://github.com/FlagOpen/RoboBrain2.0) | × | × |
| <br/>[Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation](https://arxiv.org/abs/2508.13998) | <img width="300" alt="image" src="images/Embodied-R1.png"> | 2025-8 | [Github](https://github.com/pickxiguapi/Embodied-R1) | √ | √ |
| <br/>[Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces](https://arxiv.org/abs/2506.00123) | <img width="300" alt="image" src="images/VeBrain.png"> | 2025-5 | [Github](https://github.com/OpenGVLab/VeBrain) | × | × |
| <br/>[RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/pdf/2506.04308) | <img width="300" alt="image" src="images/RoboRefer.png"> | 2025-5 | [Github](https://zhoues.github.io/RoboRefer/) | √ | √ |
| <br/>[Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning](https://arxiv.org/abs/2503.15558) | <img width="300" alt="image" src="images/Cosmos-Reason-1.png"> | 2025-5 | [Github](https://github.com/nvidia-cosmos/cosmos-reason1) | × | √ |
| <br/>[From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation (Embodied-FSD)](https://arxiv.org/pdf/2505.08548) | <img width="300" alt="image" src="images/Embodied-FSD.png"> | 2025-5 | [Github](https://github.com/pickxiguapi/Embodied-FSD) | √ | × |
| <br/>[SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models (NeurIPS'24)](https://arxiv.org/abs/2406.01584) | <img width="300" alt="image" src="images/SpatialRGPT.png"> | 2024-12 | [Github](https://github.com/AnjieCheng/SpatialRGPT) | √ | × |


## Dataset
|              Title              |                    Introduction                    |     Date     |      Code      |    Benchmark    |
| :-----------------------------: | :------------------------------------------------: | :----------: | :------------: | :------------: |
| <br/>[Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets](https://arxiv.org/abs/2505.15517) | <img width="300" alt="image" src="images/Robo2VLM.png"> | 2025-6 | [Github](https://berkeleyautomation.github.io/robo2vlm/) | √ |
| <br/>[RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics](https://arxiv.org/abs/2411.16537) | <img width="300" alt="image" src="images/RoboSpatial.png"> | 2025-5 | [Github](https://github.com/NVlabs/RoboSpatial) | √ |
| <br/>[Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/abs/2505.17015) | <img width="300" alt="image" src="images/MultiSPA.png"> | 2025-5 | [Github](https://github.com/facebookresearch/Multi-SpatialMLLM) | √ |



## Benchmarks
|              Title              |                    Introduction                    |     Date     |      Code      |    SFT Data    |
| :-----------------------------: | :------------------------------------------------: | :----------: | :------------: | :------------: |
| <br/>[Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/abs/2504.15280) | <img width="300" alt="image" src="images/All-Angles-Bench.png"> | 2025-8 | [Github](https://github.com/Chenyu-Wang567/All-Angles-Bench/tree/main) | × |
| <br/>[Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces](https://arxiv.org/abs/2412.14171) | <img width="300" alt="image" src="images/VSI.png"> | 2025-7 | [Github](https://github.com/vision-x-nyu/thinking-in-space) | × |
| <br/>[Embodied Reasoning QA Evaluation Dataset](https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf) | <img width="300" alt="image" src="images/ERQA.png"> | 2025-5 | [Github](https://github.com/embodiedreasoning/ERQA) | × |
| <br/>[SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models](https://arxiv.org/abs/2412.07755) | <img width="300" alt="image" src="images/SAT.png"> | 2025-4 | [Github](https://github.com/arijitray1993/SAT) | √ |
| <br/>[Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/abs/2406.16860) | <img width="300" alt="image" src="images/VSR.png"> | 2024-10 | [Github](https://github.com/cambrian-mllm/cambrian) | √ |
| <br/>[EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models](https://arxiv.org/abs/2406.05756) | <img width="300" alt="image" src="images/EmbSpatial.png"> | 2024-6 | [Github](https://github.com/mengfeidu/EmbSpatial-Bench) | √ |
| <br/>[BLINK: Multimodal Large Language Models Can See but Not Perceive](https://arxiv.org/abs/2404.12390) | <img width="300" alt="image" src="images/BLINK.png"> | 2024-4 | [Github](https://github.com/zeyofu/BLINK_Benchmark) | × |
| <br/>[OpenEQA: Embodied Question Answering in the Era of Foundation Models](https://open-eqa.github.io/assets/pdfs/paper.pdf) | <img width="300" alt="image" src="images/open-eqa.png"> | 2024-4 | [Github](https://github.com/facebookresearch/open-eqa) | × |
| <br/>[VSR: Visual Spatial Reasoning](https://arxiv.org/abs/2205.00363) | <img width="300" alt="image" src="images/VSR.png"> | 2023-5 | [Github](https://github.com/cambridgeltl/visual-spatial-reasoning) | √ |


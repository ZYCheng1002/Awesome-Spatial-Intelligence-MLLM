<div align="center">
    <h1>Awesome Spatial Intelligence in MLLM</h1>
    <img src=https://img.shields.io/github/stars/ZYCheng1002/Awesome-Spatial-Intelligence-MLLM.svg?style=social >
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg"/></a>
</div>

This is a collection of research papers about Spatial Intelligence in MLLM. 

If you wish to include your paper, update any details (e.g., code URLs, conference information), or share suggestions, please feel free to submit a pull request, send me an email, or leave a comment in the Issues section. Your input is greatly appreciated!


* Methods  
Focus on collecting relevant work centered around model training. Some may include datasets and benchmarks, which will be annotated as much as possible.
* Dataset  
Focus on collecting datasets related to spatial intelligence. Some may involve SFT (Supervised Fine-Tuning) processes.
* Benchmark  
Focus on collecting relevant evaluation benchmarks.

## Survey
|              Title              |      Date       |       Note       |
| :-----------------------------: | :-------------: | :--------------: |
| <br/>[Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI](https://arxiv.org/pdf/2407.06886) | 2025-9 | Robot |

## Methods
|              Title              |    Date     |      Code      |      Note      |
| :-----------------------------: | :---------: | :------------: | :------------: |
| <br/>[Robix: A Unified Model for Robot Interaction, Reasoning and Planning](https://arxiv.org/abs/2509.01106) | 2025-9 | - | Robot |
| <br/>[RoboBrain 2.0: See Better. Think Harder. Do Smarter.](https://arxiv.org/abs/2507.02029) | 2025-8 | [Github](https://github.com/FlagOpen/RoboBrain2.0) | Robot |
| <br/>[Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation](https://arxiv.org/abs/2508.13998) | 2025-8 | [Github](https://github.com/pickxiguapi/Embodied-R1) | Robot |
| <br/>[Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces](https://arxiv.org/abs/2506.00123) | 2025-5 | [Github](https://github.com/OpenGVLab/VeBrain) | Robot |
| <br/>[RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/pdf/2506.04308) | 2025-5 | [Github](https://zhoues.github.io/RoboRefer/) | Robot |
| <br/>[Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning](https://arxiv.org/abs/2503.15558) | 2025-5 | [Github](https://github.com/nvidia-cosmos/cosmos-reason1) | - |
| <br/>[From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation (Embodied-FSD)](https://arxiv.org/pdf/2505.08548) | 2025-5 | [Github](https://github.com/pickxiguapi/Embodied-FSD) | Robot |
| <br/>[SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models (NeurIPS'24)](https://arxiv.org/abs/2406.01584) | 2024-12 | [Github](https://github.com/AnjieCheng/SpatialRGPT) | - |


## Dataset
|              Title              |    Date     |      Code      |      Note      |
| :-----------------------------: | :---------: | :------------: | :------------: |
| <br/>[Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets](https://arxiv.org/abs/2505.15517) | 2025-6 | [Github](https://berkeleyautomation.github.io/robo2vlm/) | Robot |
| <br/>[RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics](https://arxiv.org/abs/2411.16537) | 2025-5 | [Github](https://github.com/NVlabs/RoboSpatial) | - |
| <br/>[Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/abs/2505.17015) | 2025-5 | [Github](https://github.com/facebookresearch/Multi-SpatialMLLM) | - |



## Benchmarks
|              Title              |    Date     |      Code      |      Note      |
| :-----------------------------: | :---------: | :------------: | :------------: |
| <br/>[Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/abs/2504.15280) | 2025-8 | [Github](https://github.com/Chenyu-Wang567/All-Angles-Bench/tree/main) | - |
| <br/>[Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces](https://arxiv.org/abs/2412.14171) | 2025-7 | [Github](https://github.com/vision-x-nyu/thinking-in-space) | - |
| <br/>[Embodied Reasoning QA Evaluation Dataset](https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf) | 2025-5 | [Github](https://github.com/embodiedreasoning/ERQA) | Robot |
| <br/>[SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models](https://arxiv.org/abs/2412.07755) | 2025-4 | [Github](https://github.com/arijitray1993/SAT) | - |
| <br/>[Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/abs/2406.16860) | 2024-10 | [Github](https://github.com/cambrian-mllm/cambrian) | - |
| <br/>[EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models](https://arxiv.org/abs/2406.05756) | 2024-6 | [Github](https://github.com/mengfeidu/EmbSpatial-Bench) | - |
| <br/>[BLINK: Multimodal Large Language Models Can See but Not Perceive](https://arxiv.org/abs/2404.12390) | 2024-4 | [Github](https://github.com/zeyofu/BLINK_Benchmark) | - |
| <br/>[OpenEQA: Embodied Question Answering in the Era of Foundation Models](https://open-eqa.github.io/assets/pdfs/paper.pdf) | 2024-4 | [Github](https://github.com/facebookresearch/open-eqa) | - |
| <br/>[VSR: Visual Spatial Reasoning](https://arxiv.org/abs/2205.00363) | 2023-5 | [Github](https://github.com/cambridgeltl/visual-spatial-reasoning) | - |

